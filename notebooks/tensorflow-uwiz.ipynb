{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2cf830ea6d6b778",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a573bc08af0efb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import uncertainty_wizard as uwiz\n",
    "import matplotlib.pyplot as plt\n",
    "from uncertainty_wizard.models.stochastic_utils.layers import UwizBernoulliDropout, UwizGaussianDropout, UwizGaussianNoise\n",
    "from uncertainty_wizard.models._stochastic._stochastic_mode import StochasticMode\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "257d5a2fba7f1695"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'Number og GPUs Available: {len(gpus)}')\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, 'Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPUs found. Please check your TensorFlow installation and GPU setup')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5af4e221598c486",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41bdb8c3f863bb39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Load and preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9ab9d10768a86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load mnist data \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Lenght of training samples\n",
    "print('Lenght of training samples: ', len(x_train))\n",
    "\n",
    "# Lenght of test samples\n",
    "print('\\nLenght of test samples: ', len(x_test))\n",
    "\n",
    "# Shape \n",
    "print('\\nShape: ', x_train[0].shape)\n",
    "print('\\n-----------------------------------------------------------------')\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = (x_train.astype('float32') / 255).reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = (x_test.astype('float32') / 255).reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model = uwiz.models.StochasticSequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(UwizBernoulliDropout(0.5, stochastic_mode=StochasticMode()))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "plt.matshow(x_train[0])\n",
    "plt.matshow(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af690cf",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "                               tf.keras.callbacks.TensorBoard(log_dir='data/logs', histogram_freq=1)])\n",
    "\n",
    "# Save the model\n",
    "model.inner.save_weights('./data/model/mnist_model_stochastic_weights.h5')\n",
    "print(\"\\nTraining completed, model weights saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070ad9c",
   "metadata": {},
   "source": [
    "## 4. Plot Traning & Validation Accuracy and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde9074a47b1f01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206280fcffe7299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the saved model weights\n",
    "model.inner.load_weights('./data/model/mnist_model_stochastic_weights.h5')\n",
    "print(\"Model weights loaded from disk\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy', test_acc)\n",
    "print('\\nTest loss', test_loss)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = tf.reduce_mean(tf.square(y_pred - y_test))\n",
    "print(f'\\nMSE: {mse:.4f}')\n",
    "\n",
    "\n",
    "num_samples = 25\n",
    "predictions = model.predict(x_test[:num_samples])\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Adversarial\n",
    "\n",
    "#### Cleverhans\n",
    "\n",
    "https://github.com/cleverhans-lab/cleverhans\n",
    "\n",
    "https://arxiv.org/pdf/1611.01236.pdf\n",
    "\n",
    "\n",
    "**SparseCategoricalAccuray** is a way to check how often a model correctly predicts which category (class) each data point belongs to. It's \"sparse\" because the true categories are given as a single numbers like (0,1,2), not lists or vectors.\n",
    "\n",
    "**SparseCategoricalCrossentropy** measures how well the model's predicted probabilities match the actual categories. It's like a score for the model's errors; the model tries to get this score as low as possible during training to make better predictions.\n",
    "\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/andantillon/cleverhans/blob/master/tutorials/future/tf2/notebook_tutorials/mnist_fgsm_tutorial.ipynb#scrollTo=O4teWeVYvUGQ"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d60b49a0f7bb02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if y_test.ndim > 1:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "test_accuracy = SparseCategoricalAccuracy()\n",
    "\n",
    "# Evaluate on clean examples\n",
    "for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128):\n",
    "    predictions = model.predict(x_batch)\n",
    "    test_accuracy.update_state(y_batch, predictions)\n",
    "    \n",
    "accuracy_clean = test_accuracy.result().numpy() * 100\n",
    "print(f'\\nTest accuracy on clean examples: {accuracy_clean}')\n",
    "\n",
    "# Adversarial evaluation settings\n",
    "eps = 0.3\n",
    "test_accuracy_fgsm = SparseCategoricalAccuracy()\n",
    "test_accuracy_pgd = SparseCategoricalAccuracy()\n",
    "\n",
    "# Evaluate on adversarial examples (FGSM and PGD)\n",
    "for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128):\n",
    "    # FGSM examples\n",
    "    x_adv_fgsm = fast_gradient_method(model.inner, x_batch, eps, np.inf)\n",
    "    predictions_fgsm = model.predict(x_adv_fgsm)\n",
    "    test_accuracy_fgsm.update_state(y_batch, predictions_fgsm)\n",
    "    \n",
    "    # PGD examples\n",
    "    x_adv_pdg = projected_gradient_descent(model.inner, x_batch, eps, 0.01, 40, np.inf)\n",
    "    predictions_pdg = model.predict(x_adv_pdg)\n",
    "    test_accuracy_pgd.update_state(y_batch, predictions_pdg)\n",
    "    \n",
    "accuracy_fgsm = test_accuracy_fgsm.result().numpy() * 100\n",
    "accuracy_pdg = test_accuracy_pgd.result().numpy() * 100\n",
    "\n",
    "print(f'\\nTest accuracy on FGSM adversarial examples: {accuracy_fgsm}')\n",
    "print(f'Test accuracy on PDG adversarial examples: {accuracy_pdg}')\n",
    "\n",
    "accuracies = [accuracy_clean, accuracy_fgsm, accuracy_pdg]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43a1a46505b56c48",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eebcd005",
   "metadata": {},
   "source": [
    "## 7. Plot Predictions, Confusion Matrix, Adversarial examples and Comparison between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'True: {y_true[i]}, Predicted: {predicted_labels[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=[str(i) for i in range(10)],\n",
    "            yticklabels=[str(i) for i in range(10)])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Clean and Adversarial examples\n",
    "\n",
    "#x_adv_fgsm = fast_gradient_method(model.inner, x_test[:num_samples], eps, np.inf)\n",
    "#predictions_clean = np.argmax(model.predict(x_test[:num_samples]), axis=1)\n",
    "#predictions_adv = np.argmax(model.predict(x_adv_fgsm), axis=1)\n",
    "\n",
    "# Generate FGSM adversarial examples\n",
    "x_adv_fgsm = fast_gradient_method(model.inner, x_test[:num_samples], eps, np.inf)\n",
    "predictions_fgsm = np.argmax(model.predict(x_adv_fgsm), axis=1)\n",
    "\n",
    "# Generate PGD adversarial examples\n",
    "x_adv_pgd = projected_gradient_descent(model.inner, x_test[:num_samples], eps, 0.01, 40, np.inf)\n",
    "predictions_pgd = np.argmax(model.predict(x_adv_pgd), axis=1)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "#plt.figure(figsize=(2 * num_samples, 6))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Plot clean images\n",
    "        plt.subplot(3, num_samples, i + 1)\n",
    "        plt.imshow(x_test[i], cmap='gray')  # If x_test is not already in 28x28, reshape is needed\n",
    "        plt.title(f'Clean\\nPred: {np.argmax(model.predict(x_test[i:i+1]), axis=1)[0]}', fontsize=9)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot FGSM adversarial images\n",
    "        plt.subplot(3, num_samples, num_samples + i + 1)\n",
    "        plt.imshow(x_adv_fgsm[i], cmap='gray')\n",
    "        plt.title(f'FGSM\\nPred: {predictions_fgsm[i]}', fontsize=9)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot PGD adversarial images\n",
    "        plt.subplot(3, num_samples, 2 * num_samples + i + 1)\n",
    "        plt.imshow(x_adv_pgd[i], cmap='gray')\n",
    "        plt.title(f'PGD\\nPred: {predictions_pgd[i]}', fontsize=9)\n",
    "        plt.axis('off')\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filename = f'pred_clean_vs_adv_{timestamp}.png'\n",
    "plot_dir = './data/plots//evaluate'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(plot_dir, filename))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compare\n",
    "plt.figure(figsize=(8, 6))\n",
    "bar_positions = np.arange(len(accuracies))\n",
    "plt.bar(bar_positions, accuracies, color=['blue', 'green', 'red'])\n",
    "plt.xticks(bar_positions, labels=['Clean', 'FGSM', 'PGD'])\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy: Clean vs Adversarial Examples')\n",
    "plt.ylim(0, 110)\n",
    "\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 2, f'{acc:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b725c7e1d3e66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 8. Analyze the Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dce07d4d720e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the saved model weights \n",
    "model.inner.load_weights('./data/model/mnist_model_stochastic_weights.h5')\n",
    "print(\"Model weights loaded from disk\")\n",
    "\n",
    "# Perform uncertainty quantification\n",
    "quantifiers = ['pcs', 'mean_softmax']\n",
    "results = model.predict_quantified(x_test,\n",
    "                                    quantifier=quantifiers,\n",
    "                                    batch_size=64,\n",
    "                                    sample_size=32,\n",
    "                                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0bf8a51eadd8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 9. Plots for Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the uncertainty distribution\n",
    "uncertainty_scores = results[1][1]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(uncertainty_scores, bins=50, alpha=0.7, color='blue')\n",
    "plt.title('Distribution of Uncertainty Scores')\n",
    "plt.xlabel('Uncertainty Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the predictive confidence score\n",
    "confidence_scores = results[1][0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(confidence_scores, bins=50, alpha=0.7, color='green')\n",
    "plt.title('Distribution of Predictive Confidence Scores')\n",
    "plt.xlabel('Predictive Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the predictive confidence score vs uncertainty score\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(confidence_scores, uncertainty_scores, alpha=0.5, color='red')\n",
    "plt.title('Predictive Confidence Score vs Uncertainty Score')\n",
    "plt.xlabel('Predictive Confidence Score')\n",
    "plt.ylabel('Uncertainty Score')\n",
    "plt.show()\n",
    "\n",
    "# Plot PCS and Mean Softmax scores for a subset of the test set data  \n",
    "#qualitative, visual correlation between each image and its corresponding scores.\n",
    "num_samples = 25\n",
    "pcs_scores, mean_softmax_scores = results[0][1], results[1][1]\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'PCS: {pcs_scores[i]:.2f}\\nMean Softmax: {mean_softmax_scores[i]:.2f}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of PCS and Mean Softmax scores\n",
    "pcs_scores = results[0][1]\n",
    "mean_softmax_scores = results[1][1]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(pcs_scores, bins=50, alpha=0.7)\n",
    "plt.xlabel('PCS Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of PCS Scores')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mean_softmax_scores, bins=50, alpha=0.7)\n",
    "plt.xlabel('Mean Softmax Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Mean Softmax Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a83521f2d6149",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs_scores = results[0][1][:num_samples]  \n",
    "mean_softmax_scores = results[1][1][:num_samples]\n",
    "\n",
    "# Plot PCS scores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(num_samples), pcs_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('PCS Score')\n",
    "plt.title('PCS Scores for Test Samples')\n",
    "\n",
    "# Plot Mean Softmax scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(num_samples), mean_softmax_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Mean Softmax Score')\n",
    "plt.title('Mean Softmax Scores for Test Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "pcs_predictions = results[0][0]\n",
    "pcs_confidences = results[0][1]\n",
    "mean_softmax_predictions = results[1][0]\n",
    "mean_softmax_confidences = results[1][1]\n",
    "\n",
    "\n",
    "plt.hist(pcs_confidences, bins=50, alpha=0.7, color='blue', label='PCS')\n",
    "plt.hist(mean_softmax_confidences, bins=50, alpha=0.7, color='green', label='Mean Softmax')\n",
    "plt.title('Distribution of Predictive Confidence Scores')\n",
    "plt.xlabel('Predictive Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(pcs_predictions, bins=50, alpha=0.7, color='blue', label='PCS Predictions')\n",
    "plt.hist(mean_softmax_predictions, bins=50, alpha=0.7, color='green', label='Mean Softmax Predictions')\n",
    "plt.title('Distribution of Predictive Confidence Scores')\n",
    "plt.xlabel('Predictive Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7d1ada5b0fe05ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
